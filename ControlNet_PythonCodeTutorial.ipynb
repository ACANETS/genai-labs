{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# This ControlNet example is for EECE.4860/5860 Intro to GenAI at UMass Lowell  \n",
    "# ported from https://thepythoncode.com/article/control-generated-images-with-controlnet-with-huggingface with minor revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_55MSKhrtss",
    "outputId": "44fe5aac-1542-4f19-b926-6fee990835e9"
   },
   "outputs": [],
   "source": [
    "%pip install -U diffusers transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xrl7OL6BrtpU",
    "outputId": "9525d6ad-4af6-4a9f-98b0-6d76308fdf13"
   },
   "outputs": [],
   "source": [
    "%pip install -U  controlnet_aux\n",
    "%pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6XPbq0FrPQG"
   },
   "source": [
    "# Open Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wEufS0RrObO",
    "outputId": "77977e89-4b72-4a5d-b39a-801953031247"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from diffusers.utils import load_image\n",
    "from tqdm import tqdm\n",
    "from torch import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662,
     "referenced_widgets": [
      "a74467c946df4fe99fba1b2751a0cad1",
      "48dabed3f2ce465ab16a704e0c6b12f7",
      "810aba56f12342fcbcb7344d08235ec8",
      "0dc0896c330d417a8b452d27699f86bf",
      "9074425e4bf04c428926f5504cd3a8de",
      "962917ac2b044e5989e88100ceb45296",
      "a212be07369b404eb9f04a9cde6543d4",
      "bf52593db97e4b5f876d22d5c27e8c0d",
      "ed79049f4f3a41899ce7744c6733bf27",
      "6fbeaf60da4746b6a1c02a211657886b",
      "9d1f8ee81b994f94882d776f6e401f02",
      "d14a6ce3bc334df5a2f319932fd9a338",
      "05ba62113d1840c38f0bcf8f291f7439",
      "117ca4fd09bb4ebeb8eae3ba5c3112f5",
      "b2fad15effb84bce8aac9b9d2bf94b19",
      "78493887a0f5466cbfe8dff7f5fbd801",
      "5850294bc56d489cb8eb90483f77254e",
      "9d2061e3643940f496528dc7ad6e6e2e",
      "52403e4bc442416fac24e0880f2a3faf",
      "38eb1d06f0094645944b5d305b575927",
      "4e264753749d48a8aab85b9f8fe30aba",
      "014e701444c54eee97d0b36ea38344ab",
      "b57cb9d64aa44f748fc6cffc6f95f5a3",
      "335f1c1d85ec4723b8796b79d518c7be",
      "e9ef39e82cf645e396daea04a1e178f1",
      "97570891602f4963bdfaacd309438ca7",
      "036af165dae748a19a3691dbe74e7a1a",
      "f9f461994c7e41aba3124dd5ee0b1525",
      "3e7b34f2b1764dbb82caaf245771abc0",
      "02f92b3266a54cc1885140f1a378d784",
      "d504edae2a2f4beb851928ca770b2701",
      "93d2d9e917d84072ae401952c12904bf",
      "636973b7927b44bbaac28737227e7599",
      "9a3d9f15c9b8495498d8910dc60d9fc0",
      "aec13914c37649feb868b8f531a07a83",
      "4b4b8aee32724ff5b0a40f9af956fe72",
      "b3967f91f621495cb1715427fcc57011",
      "d5b65c5e85704b02be5a0fa2ab7569fa",
      "fd3e1648c2b84f5f81cd7ee3d2fbc4c0",
      "25d1091281ad407c833a36c7e421e1e2",
      "15c1713762124e828b20f54684e496d0",
      "e16487cbe8d74b5baad0d41dbdeeedbb",
      "d4b746bc99074570a82175ef438cfb10",
      "8f92d6ba8bd04fabb8e6dddf0c7dfa44",
      "352a7d961cac483a82ea16cdb77a0368",
      "20285260958f4620b906e7a21f44c555",
      "85caf1e9b73e46cea1ce2c4ebf92554b",
      "23460450a39c44e488bce836dac8b997",
      "261e963568fc4a24af694276311c3b18",
      "7e03365b6d25414eb845c51aa30e3d2c",
      "199a4465ee984eff971c999393c4e497",
      "4332d0e942ea406daeca4e1bbd36d4cb",
      "44d7b2f89c984dcda69f368adbdf846f",
      "0f796401fadb4649a30effd4c66e058f",
      "33ef3f5248c14c4f8dd2d4a7bf0038a9",
      "64e440c8c82144beaec1e1d61cd5e9e0",
      "64180e5dd63840e3bb21af7aefa3cf45",
      "abb332369a9b4996b84d77eee7e5dc0a",
      "e69330feb37747c5b22d4dd215f934bd",
      "41cb4b504f3e45ea847e1cd74ae449d0",
      "c7b630dc7b8448219ca505398c12afef",
      "81f9fd794d22477f8836a11322d5325b",
      "ec96bfae8c9c48e5827755ce0e8a885f",
      "0c6beb3311554ae1b3bea23eadf8620d",
      "cb4c90617e8e4ef08052ade1d2694589",
      "6ca7d8cb9ad2472bad727eba7c993816",
      "eb5ef899d10b442ca8bd3ec6c1fbf1f3",
      "b6f79be711a94896ba08b0af7293cd04",
      "f2ec0ba35adb4084b2ef73e4d384f6fd",
      "5b6fcc006e8b47c4a416e38991201a42",
      "635b3c726b5049818d8384ad992e2550",
      "182d76403d844b96bd20c3a7c49b4357",
      "c81fe2532ac14ae6a5411a866f8e42ac",
      "9ef7b0ea4e4e46b18953056deaa6cf95",
      "2d76832bc7834ef6b683ef6829f83861",
      "2caabb6f82ef46ecbda6c0baf215d72a",
      "f1e983f41294422087b5be9a9df11fa3",
      "9b78deeeaec149339134750de6ba1fcf",
      "a207168326fd45159d8eec29ea91ccac",
      "425d719b752b4a1480f8c3f8b9388e50",
      "8baeb11dd15840e691eadde3f79bf4f2",
      "6b872da2a0944f5da6eb68792937311b",
      "848755aa7d4a4889b85870aac916c5ff",
      "922d64656ae0415795f9e50e79d5d5f8",
      "8ff9c129cb434f9bbab238f1779c9b0b",
      "7e5f077830e94e278b213ee5f11739d9",
      "97348ede0e464090845a56a64a53e493",
      "282f7b2aeec54a81b94da85c85c7cb75",
      "9a56fa1ecaee4be3954540e95a7c8990",
      "07ad70da82f04e6c8fc64ba90f114d04",
      "b09e0313ea6d461e99ee9e2778639310",
      "df65632706474b4d903b8f6f72438166",
      "7035d48a0c874fe7b7bd20c6b03bdf85",
      "83067a7d3f454087ac1de91bf17c7339",
      "d2b243b82d0641bc82bcc9500059043d",
      "b20aec5624874591a63b0d0f3d4daeda",
      "aa79187007a448a381f604ea8b5be0ba",
      "99836b0d3c5241f99260cd642081c31f",
      "99c6d5162b2d41b6b3d260f4b2c99b9b",
      "13ccb78b2d7141e1a982a8bea69023d5",
      "99b1e953ca924df2b6bfb0f9780d2e21",
      "57420a6e093c4de9845db953b8e0269b",
      "22235d8e342a47059270c8732b889d68",
      "6055afd2e29c42e09d5f923c34cf1579",
      "8477b0d8ae344f78ba17f537e0e2448f",
      "911cafa406d2409f8ac69d9c9ae0dd94",
      "589f9b88c3c746dd9aead96a6ef56fee",
      "4bf53145f39644d2bf5c3e31f9f23b1a",
      "65de6855b9944b829c4aa77bce39009f",
      "44223dcee3694e168e866af5f69aaad4",
      "7a47a1e4f4ef448e83659bf90c874eda",
      "61e2139115904b94967c220fd214b5df",
      "846c2df4629c416baea2612ea2c84a16",
      "206b5b9596ff43b68126735ad6b31929",
      "25d01861ca0c45d3a5f57932d9754834",
      "847b736c058c47f9adf366c4bb920a0d",
      "a1b45d2ecbe44a158b7a0e2f818922ae",
      "71b5693eda974ad78235b59d848304ab",
      "35aa99e07e7d469ba9a46d80bb9908bb",
      "e503940faf1e47bb911a62c6e5f33fa0",
      "03800bb9bc894d2592b3d01036769944",
      "02fe761302934b54960a8289d627d16c",
      "2db8827ffa344a4a8ac5487ff2997ae0",
      "c25de7e0bae0469d918bd6d3ea39211b",
      "2b5e8dcb60ee4f65af5505ffa7ea3ae2",
      "9b27785afb0243f5b2298cb272c510e2",
      "6ba872bd45c34e6c8202cc3722dc9285",
      "4d73b627ecc74bf7ad18f5effa245097",
      "35bd707e26044bd1a361c2fc2c11ea29",
      "ea8e9b50f98c43b58d7faccbfd6821cb",
      "940a28141ae14329831b67e91ff1091a",
      "f1f865efa3a6448ea9c930a1f08c50bf",
      "ede9d9c5d44146849e44c3b1476299eb",
      "d8424b9018444e088fe5908db8a9db12",
      "f00bbbb986ed49e79560422ba4c379c8",
      "108208309c4b47aa8eee3a1321fc81ed",
      "d0c00b0f105741f9a155e941dd5d704f",
      "16e7c873157c444bb0ca1b92e85b4701",
      "4f4bfc7bd24148e6ac524b57cc6d8020",
      "2a8de4ddaf4747ceb0a153851cd765ca",
      "c3e33bee752849c692d00ceabd561feb",
      "97dcb5d933594743aea62d09aa27d30d",
      "f17e8c0187db4242af4a803aed44fde9",
      "483e47a822fe43d6ae4c114bfbb8be8a",
      "c905c8d69bcb404aa39ca499c96549e3",
      "bd5c5827932b4dabbac38adb33544e48",
      "dddba90edf87434aa404eea38eaa63f2",
      "a32f66c9b59e4ae2bca41c1c582119e7",
      "e8cbb2d5adae4b439639631ea833be72",
      "06f53eb5d18941d8a947bcb21676e9c7",
      "455bc344fe60421da1ac931657cbd162",
      "88491458b25e4ae5a66afeaae7f76c12",
      "32bb55abea4f4ea2a7ea8a28a45eeb16",
      "90d2a57096964b7a9e2aef14ec54e2a7",
      "a526959014a644cba47969ebba62fa7c",
      "ceb41aa2ac6c4c1982598a5085f7a977",
      "8dd1b3d2ce124d2a92d69bb1897a078e",
      "c7eafdcd51dc419c99b3248093fbfbff",
      "5eca9f78607349a9893b0c88f3c53181",
      "719559db7a7943fbb823795a1697a10c",
      "445925a5c9da455c90fd9225ab521c1f",
      "2898c9af6aea4c79a4f9e89e24275083",
      "c34ca1ea918344188c3241400aa3e775",
      "dd9945400a2c413589af32550427f92d",
      "8328f2c010f4426fbfef22482701f516",
      "edaf046faeb8459bbe937cae180bfc4e",
      "8cb0f3ae820947a5984dd422b33776dd",
      "ea6ed2a4fd9c49579f25b2f0e9af3f3c",
      "41b442152042436ea0703d3c1181824d",
      "f3af911e563d47eca89517bdc434c911",
      "b5462a2b462147899d82f6b4617cf781",
      "d84093973fe84b06bb05f125ab7a11bb",
      "3cdb72138ffe4261a6cfcd9bcf4776c4",
      "3bdc5d0f6e324afaae6d5a4aabff98d2",
      "18a72653fc284d29be02e4d2a477ecdd",
      "ff1597bd843b4770b93ffa71f0b05da4",
      "b3bd2d853e02483080ff7dfbd5b448e0",
      "55f31f27ce954a0c8c0d1e63938c7bc0",
      "40ef1d0b44164265bea4eea32ce1b907",
      "3da34455151b42319fc32e56afeb7959",
      "1bf842268bf64cadbf2bda6da0d4a690",
      "452c129fabca48efa832b81481703bab",
      "6e327baf7a9144fcaff7c6317d018862",
      "72d25c37112c403eaf801833a03a47b2",
      "8c4e2621ec244a2db21262a8ed0e4ab7",
      "965fb38eae6a45678d87fc3e24eb5241",
      "0da3c6cd22da482c88af347dcbdf0d8e",
      "f7dd7600e3374757bdb1dad3d2e29d4e",
      "dffc9bcfd178487e93b34f91eae3b8e8",
      "7016696c087c4b3d85d5928cca064add",
      "f4accfef05c24411bbd0fae1f7fbf7cb",
      "941873b1d19b4c2da615ed461fc6cbc7",
      "6ea8901001af4f2aa0039b18f5e758a4",
      "ecc46cbc78fe4d9f8af50970839d3891",
      "7301b1053b6446da9756caeb59d5be8b",
      "94fba57a53d04861ab5a889193d27f83",
      "aafa0b9b142a44f79368f411710ea55c",
      "2aeb8135603c46c1aa80e2c3580b897f",
      "a2630966b93643e99721ea631339d088",
      "6ca84bcd6bea4a92a80d50351aa1be4f",
      "d2a03fa596c34b12a1aed264e8d08077",
      "74702e35e09048eabfb470dc85fb5be4",
      "4276b44c60b64716ab7ad52d8354bc1f",
      "87fcb7d5949d440dbe3c18e7059e65ad",
      "173d85b54dd4419883fc83cfa2beacc5",
      "cd4f33b831ff4bd4afabbdec87b9bc8d",
      "79b1b7bd05714b528d9a617b1c875dfe",
      "d6a728463cce43d2a0c8573e5faf2dde",
      "e59d5e76445943c39cdb32721727d3e8"
     ]
    },
    "id": "ts8fiPLordOD",
    "outputId": "08a7450f-8e53-48a4-9c2d-2994353c3140"
   },
   "outputs": [],
   "source": [
    "# load the openpose model\n",
    "openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\n",
    "\n",
    "# load the controlnet for openpose\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-openpose\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# define stable diffusion pipeline with controlnet\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n",
    ").to(\"xpu\")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "f_HH_n8WrdQN",
    "outputId": "2432a1d8-cdec-4b0e-ca8e-d62ce008136e"
   },
   "outputs": [],
   "source": [
    "image_input = load_image(\"https://cdn.pixabay.com/photo/2016/05/17/22/19/fashion-1399344_640.jpg\")\n",
    "image_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "id": "AUJikHhlrdSX",
    "outputId": "398bd929-ff3e-4f05-cad6-a3079b911d75"
   },
   "outputs": [],
   "source": [
    "image_pose = openpose(image_input)\n",
    "image_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817,
     "referenced_widgets": [
      "b5a39dc5eccf4747ade601d25681c2b2",
      "1ce482eed7dd4a62b5c97b7fae38f392",
      "f566fa59f127470087bb68beea6ee8fb",
      "6c0b26e65230468cb5ef11d3d0bf1bd3",
      "aa4097be74c3412eb9571ed6ef250828",
      "f02818f8f3d54f72961c51e8fe526dc2",
      "8d2da5062f414a7f8c98facdd7d2bad0",
      "2fa71efbdefa4607b37d985b272fcffa",
      "e20f4ef3c100484cae5ba0a570be604c",
      "f48bc2b8666b4e48a6f8cad4cda2530d",
      "29e96ee99a714e2c86a0d68be1a31055"
     ]
    },
    "id": "0MfsiN_Jri4G",
    "outputId": "5286a2ac-13a3-4c35-80ab-a9968c9c8602"
   },
   "outputs": [],
   "source": [
    "image_output = pipe(\"A professional photograph of a male fashion model\", image_pose, num_inference_steps=20).images[0]\n",
    "image_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEiobo68Kzso"
   },
   "source": [
    "# Custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0y9iWI9cK17f"
   },
   "outputs": [],
   "source": [
    "class ControlNetDiffusionPipelineCustom:\n",
    "    \"\"\"custom implementation of the ControlNet Diffusion Pipeline\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vae,\n",
    "                 tokenizer,\n",
    "                 text_encoder,\n",
    "                 unet,\n",
    "                 controlnet,\n",
    "                 scheduler,\n",
    "                 image_processor,\n",
    "                 control_image_processor):\n",
    "\n",
    "        self.vae = vae\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = text_encoder\n",
    "        self.unet = unet\n",
    "        self.scheduler = scheduler\n",
    "        self.controlnet = controlnet\n",
    "        self.image_processor = image_processor\n",
    "        self.control_image_processor = control_image_processor\n",
    "        self.device = 'xpu' if torch.xpu.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "    def get_text_embeds(self, text):\n",
    "        \"\"\"returns embeddings for the given `text`\"\"\"\n",
    "\n",
    "        # tokenize the text\n",
    "        text_input = self.tokenizer(text,\n",
    "                                    padding='max_length',\n",
    "                                    max_length=tokenizer.model_max_length,\n",
    "                                    truncation=True,\n",
    "                                    return_tensors='pt')\n",
    "        # embed the text\n",
    "        with torch.no_grad():\n",
    "            text_embeds = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
    "        return text_embeds\n",
    "\n",
    "\n",
    "\n",
    "    def get_prompt_embeds(self, prompt):\n",
    "        \"\"\"returns prompt embeddings based on classifier free guidance\"\"\"\n",
    "\n",
    "        if isinstance(prompt, str):\n",
    "            prompt = [prompt]\n",
    "        # get conditional prompt embeddings\n",
    "        cond_embeds = self.get_text_embeds(prompt)\n",
    "        # get unconditional prompt embeddings\n",
    "        uncond_embeds = self.get_text_embeds([''] * len(prompt))\n",
    "        # concatenate the above 2 embeds\n",
    "        prompt_embeds = torch.cat([uncond_embeds, cond_embeds])\n",
    "        return prompt_embeds\n",
    "\n",
    "\n",
    "    def transform_image(self, image):\n",
    "        \"\"\"convert image from pytorch tensor to PIL format\"\"\"\n",
    "\n",
    "        image = self.image_processor.postprocess(image, output_type='pil')\n",
    "        return image\n",
    "\n",
    "\n",
    "\n",
    "    def get_initial_latents(self, height, width, num_channels_latents, batch_size):\n",
    "        \"\"\"returns noise latent tensor of relevant shape scaled by the scheduler\"\"\"\n",
    "\n",
    "        image_latents = torch.randn((batch_size,\n",
    "                                   num_channels_latents,\n",
    "                                   height // 8,\n",
    "                                   width // 8)).to(self.device)\n",
    "        # scale the initial noise by the standard deviation required by the scheduler\n",
    "        image_latents = image_latents * self.scheduler.init_noise_sigma\n",
    "        return image_latents\n",
    "\n",
    "\n",
    "\n",
    "    def denoise_latents(self,\n",
    "                        prompt_embeds,\n",
    "                        controlnet_image,\n",
    "                        timesteps,\n",
    "                        latents,\n",
    "                        guidance_scale=7.5):\n",
    "        \"\"\"denoises latents from noisy latent to a meaningful latent as conditioned by controlnet\"\"\"\n",
    "\n",
    "        # use autocast for automatic mixed precision (AMP) inference\n",
    "        with autocast('xpu'):\n",
    "            for i, t in tqdm(enumerate(timesteps)):\n",
    "                # duplicate image latents to do classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2)\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                control_model_input = latents\n",
    "                controlnet_prompt_embeds = prompt_embeds\n",
    "\n",
    "                # get output from the control net blocks\n",
    "                down_block_res_samples, mid_block_res_sample = self.controlnet(\n",
    "                    control_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=controlnet_prompt_embeds,\n",
    "                    controlnet_cond=controlnet_image,\n",
    "                    conditioning_scale=1.0,\n",
    "                    return_dict=False,\n",
    "                )\n",
    "\n",
    "                # predict noise residuals\n",
    "                with torch.no_grad():\n",
    "                    noise_pred = self.unet(\n",
    "                        latent_model_input,\n",
    "                        t,\n",
    "                        encoder_hidden_states=prompt_embeds,\n",
    "                        down_block_additional_residuals=down_block_res_samples,\n",
    "                        mid_block_additional_residual=mid_block_res_sample,\n",
    "                    )['sample']\n",
    "\n",
    "                # separate predictions for unconditional and conditional outputs\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "\n",
    "                # perform guidance\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                # remove the noise from the current sample i.e. go from x_t to x_{t-1}\n",
    "                latents = self.scheduler.step(noise_pred, t, latents)['prev_sample']\n",
    "\n",
    "        return latents\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_controlnet_image(self,\n",
    "                                 image,\n",
    "                                 height,\n",
    "                                 width):\n",
    "        \"\"\"preprocesses the controlnet image\"\"\"\n",
    "\n",
    "        # process the image\n",
    "        image = self.control_image_processor.preprocess(image, height, width).to(dtype=torch.float32)\n",
    "        # send image to GPU\n",
    "        image = image.to(self.device)\n",
    "        # repeat the image for classifier free guidance\n",
    "        image = torch.cat([image] * 2)\n",
    "        return image\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self,\n",
    "                 prompt,\n",
    "                 image,\n",
    "                 num_inference_steps=20,\n",
    "                 guidance_scale=7.5,\n",
    "                 height=512, width=512):\n",
    "        \"\"\"generates new image based on the `prompt` and the `image`\"\"\"\n",
    "\n",
    "        # encode input prompt\n",
    "        prompt_embeds = self.get_prompt_embeds(prompt)\n",
    "\n",
    "        # prepare image for controlnet\n",
    "        controlnet_image = self.prepare_controlnet_image(image, height, width)\n",
    "        height, width = controlnet_image.shape[-2:]\n",
    "\n",
    "        # prepare timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "\n",
    "        # prepare the initial image in the latent space (noise on which we will do reverse diffusion)\n",
    "        num_channels_latents = self.unet.config.in_channels\n",
    "        batch_size = prompt_embeds.shape[0] // 2\n",
    "        latents = self.get_initial_latents(height, width, num_channels_latents, batch_size)\n",
    "\n",
    "        # denoise latents\n",
    "        latents = self.denoise_latents(prompt_embeds,\n",
    "                                       controlnet_image,\n",
    "                                       timesteps,\n",
    "                                       latents,\n",
    "                                       guidance_scale)\n",
    "\n",
    "        # decode latents to get the image into pixel space\n",
    "        latents = latents.to(torch.float16) # change dtype of latents since\n",
    "        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "        # convert to PIL Image format\n",
    "        image = image.detach() # detach to remove any computed gradients\n",
    "        image = self.transform_image(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kugjwBC3K4JK"
   },
   "outputs": [],
   "source": [
    "# We can get all the components from the ControlNet Diffusion Pipeline (the one implemented by Hugging Face as well)\n",
    "vae = pipe.vae\n",
    "tokenizer = pipe.tokenizer\n",
    "text_encoder = pipe.text_encoder\n",
    "unet = pipe.unet\n",
    "controlnet = pipe.controlnet\n",
    "scheduler = pipe.scheduler\n",
    "image_processor = pipe.image_processor\n",
    "control_image_processor = pipe.control_image_processor\n",
    "\n",
    "print(text_encoder.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ooKnlkVK4LV"
   },
   "outputs": [],
   "source": [
    "custom_pipe = ControlNetDiffusionPipelineCustom(vae, tokenizer, text_encoder, unet, controlnet, scheduler, image_processor, control_image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "BYW8mEH2K4NY",
    "outputId": "6e610454-e4c4-4c9d-c091-23cf9cbecec9"
   },
   "outputs": [],
   "source": [
    "# sample image 1\n",
    "images_custom = custom_pipe(\"a fashion model wearing a beautiful dress\", image_pose, num_inference_steps=20)\n",
    "images_custom[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "w4XGMCsUK4RA",
    "outputId": "59e49ba7-d3c3-4fd7-bf0c-ed9ac23695dd"
   },
   "outputs": [],
   "source": [
    "# sample image 2\n",
    "images_custom = custom_pipe(\"A male fashion model posing in a museum\", image_pose, num_inference_steps=20)\n",
    "images_custom[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "GU9P7QMacZTW",
    "outputId": "b3c7e3aa-04b1-47c9-e29d-a8b68457b8e8"
   },
   "outputs": [],
   "source": [
    "# sample image with a different prompt\n",
    "images_custom = custom_pipe(\"A professional ice skater wearing a dark blue jacket around sunset, realistic, UHD\", image_pose, num_inference_steps=20)\n",
    "images_custom[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXShSB0Fd7qd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dW5PCk0d7t7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwnnyHq3oi7O"
   },
   "source": [
    "# Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1L96sjJ_oi7P"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "import numpy as np\n",
    "from diffusers.utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566,
     "referenced_widgets": [
      "6846be1897c64311a10d421483131bf9",
      "147cd6ad4d4c4fca82a7a531684ffd2a",
      "6f88f6e516064ed1a82e1d69742d324c",
      "9998dc803aec438ab2bbdcb1edb8ab4e",
      "67f164f2226d430eb0c2cd3c63d14368",
      "4fbfe981703e4365a6e944cb9f3913d5",
      "ce5e2b3caef7421da3383cbdfc5b271f",
      "f1a22eae63cb4eb982a02ccd11422014",
      "aac46407f5d74ebbb6a58b10de9b86f4",
      "713f6c4e65a5418481d59fb756bd043c",
      "365b7759e93949dd891bacc424e797f3",
      "9e9bd7cc85b04308b20943578ad51fa7",
      "743a58e2430947808aac30953973ec71",
      "600acc1527c4482fb7307f2d423a0a01",
      "84f1832335f44d73b6a1a588f0e79a11",
      "03264d2988b849369f26e62a0a010d51",
      "a56166c024494356bc1b9a9e9a434c86",
      "e404d2f75cdc4a9397c82bca53ca3fad",
      "f16aa25f9d2b49cc87cde26cf7dc316f",
      "a263feafcec44c2eab4150af170d670a",
      "e90005c8a5664c6f80e37268666f1a9f",
      "783dca59f6a54589b6fb848cb13c5f3f",
      "72fb9eafc22a4533a62e3189f3b65b04",
      "febabb96db10490f835e4c5f6512ea61",
      "5fc991cde70546d3a665982956cca6b5",
      "f5e6af9d73ed4f3d9726bffba48540ee",
      "7027773c6c484afa9ee32448a320f948",
      "834aa09c8e984c819849676acdf25dae",
      "6224c6c1d4b145ed86564e7d62fbe4c2",
      "2dc4775bcef749b7bbbb3f71d714211c",
      "838cb3152f3c48ec814de02def9b75f2",
      "0a6ed397228f4a1fafe9dfecc4c39ef6",
      "e2eaf1b78df545feb426d75ec556f8a3",
      "db3e2ef136864292b6cc51ede3713a4d",
      "554525ce609044b68804e97c6b45c510",
      "97dfae674a3b415d9015760abea61968",
      "e4394ac60eef460e8f37fac24327ab77",
      "ffd1b0965ac748a3857a947c94e192dc",
      "0d45fef5bbf447938e7621e77207e1ae",
      "681be606b86c42fd9cad68e63e2782de",
      "a0f1a81a8d8f4e05a26d7a0a61565c53",
      "7f36f3521fa14a37b0fad0702bf9a4e3",
      "51b27a7dc0d34dd09b1ea17aba1fb697",
      "2a8c5504fb034f608567f8d706389d79",
      "ebe0534f9b4c4f92ae488a8d3e5867d6",
      "e7a84d718566443bba6c79729de0518a",
      "8cec2e55fc3542789938c24adb050ca9",
      "5248a6e5d8044c53803426afaf2439af",
      "10ae3f8453744a7e9737a9a0914dfeab",
      "81b870c0f0b6482a9f1dbd3fac0d935c",
      "ff464c023f0546209b15c44a39b4b75e",
      "ec7bdbec2fc342f38039b8420a162a25",
      "dab4d54863e24f169f18999da88be46f",
      "d50a15bbc94f49c39941bacd8c7b99ba",
      "a8dcc8f3435a4870a76be1729b003b6f",
      "e19591097be54f3d954a75bf14968b46",
      "f92c9532d4644693831a67344d499ff5",
      "a283d06ba73a4fc98e18e7080e794edf",
      "5837463f803c4c93b91a21d9854ae574",
      "ca5bf86a004a45a6ae2067fdc6eef091",
      "c389029d6e9445febf2f623921140abe",
      "9eda2eb5008b4f2d8269523ee5ce2ffe",
      "44bc01157b4945b7b81401f978dce8ca",
      "e39e7bf535fb4492ae9ac75591e79287",
      "cb157284f72d428fb786527ce87d11a2",
      "24030dedd2f24e2cbfa3833249d09810",
      "b3d9135383894e049a5f6f7fc77d7b5a",
      "a97373cd0b29464dac7d2b3bfe3aa276",
      "10bfbe36a3e24b30a6e300e852ff7811",
      "af944c373890491698424b1ca1c6969b",
      "3b250841cf664fcd8a57fb25d23b29ee",
      "745c74b4ac1942c497d0bee51b2e7b17",
      "89feac2a7b7d47388548f946fff4c862",
      "6e7d3e445440469baa5aaab1cbbb9291",
      "a3ca3b9ce6b24d43bdf8361379b6d502",
      "b82b0f73fa4d4fc395d106930015c7a0",
      "bd3c80a835d242d19fe187d65e737d6c",
      "ca9e4db991b54eb986375b30bb367afa",
      "4c8fa5fd3d224ce694243ea790de8986",
      "b2efd56d5e834196818f46a7b287c2c3",
      "3097e4c3b53e4343a9ed6860b8d3d033",
      "a3fc1ea7dc724184ae564edeb2e755ba",
      "28baa1ec6f3c46309c04f2c8462b1798",
      "9cc4d939cc454a0da5eb7bdd234f69b5",
      "058220c3342b49e0bcc1f0973566ac72",
      "7db99c4ea5e14e9bb5180a5beb4cc142",
      "344e28ed2fc344f4ad5ab6f62a042631",
      "dc495adaeb53477d903ce952060e4f3f",
      "d95e3f841d4f4f278fb3adae0b11d4c3",
      "ee9448f0b9964d8fad850501e08e3c84",
      "485caac1aa1641a3b31827620ddfa3e4",
      "55fe364a57aa4eaa93cb93b889d241c5",
      "3deaa7c9f0c940c3af3b9a7c5d0c8de4",
      "93df8bff634740a29f3820042d69347f",
      "373d310d37454a409b53c30e7d7d1de5",
      "bcd48b3b056c4c14b6034ef32fdde5d5",
      "948f2f29870b4ee7974ddad6ba18397d",
      "978a0bae9c5f4ab7ac45f1f4f870fef0",
      "233120cc0399483ea17284432feb16d3",
      "183551cc7517479db8e0fffe634f2e6c",
      "90398133563a48288ebf11cf0e0e242d",
      "b6c5549bd95644ad9e553c2e59199f04",
      "48ec35653f99446db9cefabe146bb2d8",
      "84e8bc8164b948d8897a8637c226ef1c",
      "89abbaee27c441b3ab1c656806a7e67d",
      "9a46a59ffb8d44c89b067519101f7a74",
      "c0771d4d9b7242c4a1b6ea8954e9e96e",
      "682a337dc14e48908be4cf40f7320d1f",
      "6d699038776e4fad8bb74d2381109b65",
      "38dd90648c1949b196ebf27fb21436ba",
      "dde9a4c3dd824b6f8345b29f2a28ef5f",
      "6da0c63935ac4ee4bfe6a6efbedfa93d",
      "42109c92a03c45cabd7fbb0cc7cf1c4a",
      "db6ac9519e6a438ebca23446b0f2a4aa",
      "8adb835bb80941dfa3e21c9dac93d793",
      "e57b840c4d3c4abcae1b69b189701c72",
      "304e7a7a38834b3093002b6694c7cb67",
      "5056e439bb4b4bad8b0694c79138424f",
      "7d6d17a2798c41c6b27c25e1057869a3",
      "52a201ace1f647059d93e5d9406afee5",
      "43820ed672c04bbf80155b25c4e4de15",
      "b8a38cb7b914460caf5f190692f5b935",
      "633f3db9f5dd4f6984aa821a1717c03b",
      "c184aabb31504cbd8361a9ab06899438",
      "5c428d2de97f4bc28835ead1f6331e9e",
      "229a1efdf3df4fe9aac07562774b084b",
      "697d6a7fa4ee4d37bb80d624ffedf526",
      "c87c97a9ed4b493c8d6df7d11b1684ca",
      "9ee7a0fe9750457abaf427983e5866fe",
      "103ffa0973364618b4ce1d5528078062",
      "b08e7b6e25c24054ac5b2b18bcb6f2ae",
      "5532a6b6181a4aecbfca636813b661fd",
      "4f425c415dac44b3b86aa4f481254196",
      "97ffad3c1b6e4c9c9621966c028c12aa",
      "3c2af8d5617b4b878b2e68f3fc7ebff6",
      "8d27414ca68a4494ab3febc1c8a03025",
      "10f4d1976dc04c3bb651b45c08298fab",
      "eb72384f0e50444d9db03c12aca4b899",
      "e7ddc52526db41e4907a2ede8dcef7c7",
      "9f4a6f9a897c4db0a8d92bf01f3f820c",
      "88e4029063f44ce5a7e89e8f17664eeb",
      "77a73421b7ae4bfe9fcb87353a121402",
      "7bee130b66754bbc85e8454455ece425",
      "f61abe945b124e31a72254333f26c785",
      "b1da7b0efaaf4db4a7660bc5457f2bfa",
      "3e55680d33db47099479572d0c8915f2",
      "c48ca8b557154fac885edaebc4879321",
      "d4c261dab25d405ba560d4626683bccb",
      "1daa130f38a84c6d9a4149536b21927e",
      "bf14652b43264c2b884e2d19786d6f5e",
      "a94d5d8411844431bb022813d14461c0",
      "161da25b295b4afa8f8b88507818d052",
      "fd444832eb3b4399946317b54b495d52",
      "629f9b2f0b79420ba0baeb5961df73da",
      "869c22e5399145dab7e36633b910abca",
      "6e89b30bfcd04f1ebc77492eae365847",
      "7f27d4dadc2a4c4783f35193cf085593",
      "fa4ce3277cce4064aaf40849870966c1",
      "d2324949415f4df2a964046b326231a4",
      "4a3c28369fc14bbebdc749d0b3a5747b",
      "be72ef0e4b8840608fd9160a6b43075d",
      "6d8e99610d624a09a53ef0c77b3f0c25",
      "b9df58ad481f414f8240ce076cf53363",
      "3b2265f2c01844fb9c99e18c61344d69",
      "e060cf453034447dba6af95a918e3092",
      "7f49c5a346894250ad0587fa3420635a",
      "c318de49fe3a455992738cf20f7099ad",
      "3ff34cbabc564abebd12a9e5e19786b0",
      "611bde262a924746a253ab5481b9b212",
      "3ae595d0044944ea959d949d8c68d509",
      "5494938bff7d4c6da3d82f1034404362",
      "f05ad5aa13a040609336f0d6b2580be2",
      "02648abd8847434496722b0c5bb69858",
      "e3b2deb57c0644dcbf97297abf22f91c",
      "7267e3d447ef4dfa97c3fc6a560ea797",
      "2bc90ab8db7b4f979ff0cb5eb01284a6"
     ]
    },
    "id": "Dhg4rTN3oi7P",
    "outputId": "f2114cf6-bbc5-4e5e-827e-e5fc0a92351e"
   },
   "outputs": [],
   "source": [
    "# load the controlnet model for canny edge detection\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# load the stable diffusion pipeline with controlnet\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n",
    ").to(\"xpu\")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "ATEwEV-6oi7Q",
    "outputId": "055658c9-5ab9-4496-a407-f4a213f84bd8"
   },
   "outputs": [],
   "source": [
    "image_input = load_image(\"https://cdn.pixabay.com/photo/2023/06/03/16/05/spotted-laughingtrush-8037974_640.png\")\n",
    "image_input = np.array(image_input)\n",
    "\n",
    "Image.fromarray(image_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "K2c64abboi7Q",
    "outputId": "fd9de66a-d340-4b1e-bc51-7e9c96f1f80a"
   },
   "outputs": [],
   "source": [
    "# define parameters from canny edge detection\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "# do canny edge detection\n",
    "image_canny = cv2.Canny(image_input, low_threshold, high_threshold)\n",
    "\n",
    "# convert to PIL image format\n",
    "image_canny = image_canny[:, :, None]\n",
    "image_canny = np.concatenate([image_canny, image_canny, image_canny], axis=2)\n",
    "image_canny = Image.fromarray(image_canny)\n",
    "\n",
    "image_canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473,
     "referenced_widgets": [
      "27d1ad377e6c45d7b4b1c49786438015",
      "475c331703bb43adb142f6c9cc41a3bf",
      "10be4b9595c84f36aeab9593e63440e6",
      "645ebe129fd544a4b9383dfc1414cf0a",
      "51605e182e344a94a99c50fcaaef17b1",
      "c45aec98de234d3982ff83924dce8d7a",
      "942ace43ed1d48aeafc6c5e17f14e034",
      "51890b032fbb48139e11d69850a6db57",
      "30408dcf6dd14e27bab9cc06cc664c4c",
      "b539338d7bf84f50a8c57f90986a8d9a",
      "d16de73052ac42e8a8065d4dc54ebe51"
     ]
    },
    "id": "zac6FpNwoi7Q",
    "outputId": "8bf84b99-fe11-43d2-82e2-f2b35ae99d5a"
   },
   "outputs": [],
   "source": [
    "image_output = pipe(\"bird\", image_canny, num_inference_steps=20).images[0]\n",
    "image_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473,
     "referenced_widgets": [
      "48a26564f08f43dd962464feb762c232",
      "c6965bc561db4c519b751c5b7bff96a8",
      "032471548f5c45ed89d8a62db800c7fc",
      "0af102aac55747fcb8854b8b5ed2dd27",
      "a086d1c6722547d5be5939cf4284a1ef",
      "ba3cf4ae2c4e45e58abb82d2491fb7ba",
      "c64f35616d8e4b4bb129bc7aaa4ae889",
      "01fc82bb9bb84c628890b9a2349a6e6e",
      "f66573653707458f80b29a40e1193d31",
      "7b0ece2c75614540a942214d1f527f91",
      "0d3e1914b85047c7af6b6bd4fd94e197"
     ]
    },
    "id": "DXrdLeZrplMW",
    "outputId": "31bb0440-493a-4ee8-edc0-23c090f679c4"
   },
   "outputs": [],
   "source": [
    "image_output = pipe(\"a cute blue bird with colorful aesthetic feathers\", image_canny, num_inference_steps=20).images[0]\n",
    "image_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Yti9Dg8ofxd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o-9jxdtpvgi"
   },
   "source": [
    "# Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgwvAu2xpvgj"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers.utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcxFcjHspvgj",
    "outputId": "79bed3a2-27b2-411e-a748-22949d5cac62"
   },
   "outputs": [],
   "source": [
    "# load the depth estimator model\n",
    "depth_estimator = pipeline('depth-estimation')\n",
    "\n",
    "# load the controlnet model for depth estimation\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-depth\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# load the stable diffusion pipeline with controlnet\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n",
    ").to(\"xpu\")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "t0_6mNOgpvgj",
    "outputId": "8dd080e3-9fbb-4bc5-bd4c-375a4e1f18ee"
   },
   "outputs": [],
   "source": [
    "image_input = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png\")\n",
    "image_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "oTim6FQrpvgk",
    "outputId": "4cc296a7-0822-4012-a07c-232f14bb039e"
   },
   "outputs": [],
   "source": [
    "# get depth estimates\n",
    "image_depth = depth_estimator(image_input)['depth']\n",
    "\n",
    "# convert to PIL image format\n",
    "image_depth = np.array(image_depth)\n",
    "image_depth = image_depth[:, :, None]\n",
    "image_depth = np.concatenate([image_depth, image_depth, image_depth], axis=2)\n",
    "image_depth = Image.fromarray(image_depth)\n",
    "\n",
    "image_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505,
     "referenced_widgets": [
      "173d24393b854ad0a03fd02d56c7d037",
      "a33e6193984b46d4bb9b4187d726009e",
      "230461cc201f4535a9f9bcf7a3ae7669",
      "4900e42f3bfa4a6096b16d47d38687dc",
      "2112261a320b4b9297aa3edc85fa2316",
      "1c65960b291c4d92bced5bca619e8256",
      "194f16cb7fbb42a78d9237e4e6654b58",
      "b87077da3f824478ac88369582d77e8e",
      "6f9f95e2d503403f83be05364b4199cd",
      "c39f69cd4e8b4bca8e95bda3b06a74f1",
      "f8dbc4c64a0d440e9660b0d7ef083b5c"
     ]
    },
    "id": "zK5HpMNdpvgq",
    "outputId": "2b2e889d-8920-404f-e8de-e2753caebf05"
   },
   "outputs": [],
   "source": [
    "image_output = pipe(\"Darth Vader giving lecture\", image_depth, num_inference_steps=20).images[0]\n",
    "image_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505,
     "referenced_widgets": [
      "083aa6ce3274445c830c4157dc9ea4be",
      "be56ad8a63884a33a22bfb3b39da1cc0",
      "83b4c17fa697445f86f7f034fe670934",
      "c3dc31d7d2be42be88fe03fa33e5f20d",
      "2c38cd148fe74a5897c3319be42b7d7e",
      "c44f196498724b1b80b25bda832dc310",
      "069b424c2d3846b8ba69a455dbda1f88",
      "707ce79ce0da4f35ad6b8f636d2901f8",
      "261aabd2415d43809b669b49f3899504",
      "7427e45c55994a6fa304d6d58ffeb1fb",
      "2df0162802aa488dbddb9a7376954865"
     ]
    },
    "id": "ulgn37vmrUS8",
    "outputId": "aba588b6-5de1-44c5-8d96-79ac14a89f9a"
   },
   "outputs": [],
   "source": [
    "image_output = pipe(\"A realistic, aesthetic portrait style photograph of Darth Vader giving lecture, 8k, unreal engine\", image_depth, num_inference_steps=20).images[0]\n",
    "image_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uc3OBwGryX9"
   },
   "source": [
    "# Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZ8NidcHryYF"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import cv2\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "from diffusers.utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "referenced_widgets": [
      "f0ddc614bd9a40468c083db3a457e1c1",
      "a27de7fc1bfa486f9ff8829d4d4198f6",
      "47507d2cf4e04f2f927ee369c6d7b4d4",
      "e11151541738459ba16b582a9edefd2c",
      "33c23b5b6adf403dba45e9f8e64e37dc",
      "a9a5528792504d12bf153d2b02fd3847",
      "439ee7b8de924bf6ba5ee4ef22e0e659",
      "b4e865be02fb4290849b676f4dd18ed0",
      "87d4c1e6a2a047f887704f6b0a148d2e",
      "b74532a8bd8e49f2a8286a1eed72a0d3",
      "dd66e37aec9449de944edfa6dacdfc5f",
      "57216de8dfcc4ab78bfb8c41fae856b4",
      "c85b3e0dd20f414181d768cac5c9ab4e",
      "53774d0251ea4e4a884fc265a5ce561b",
      "30cdb95dd5d746b1adacaaf823b21b89",
      "bfd0a68276654b4997b802bc6d47f684",
      "dd81ef8c35384eadb1572400c775f789",
      "de80e8cbbd25433c88267a1f997772e7",
      "e116ebfa368d424ca64eea23a82ef959",
      "2b92efb0492b4af6b50e2d5754212b5c",
      "2fee0c2fcb3b4705ad4cc212d6138ec9",
      "5f012cede7a0405fbb12550072bccbc2",
      "a999af1084134026904611113b3feac6",
      "814f29773b6b40d99d03320a3f5fe7d4",
      "9f7e4357439944e392d0c9e93068c898",
      "8e31c3cc06bd4e9b92c3b1e6d5a9d61f",
      "9dc6d26cdb7945ffb31b2c381412a383",
      "36de2037f4b841afb366f27a7eb08396",
      "cb805d94485c4ff5a0aaec67e6cdb827",
      "d923cb2715554512a7faf5b5d73a4ed8",
      "4399ee6304e94797ac6f36318165d4cc",
      "7c2b45b1cf2a485b93e52b2dee04a278",
      "851c4973cb0c4fbe91536a09f006ad43",
      "7d10f1e6258943dd86d9c75aa17553c8",
      "193fa11998404439b90913f5f5b4619e",
      "0383c8a7e3384eec9205dc8f37654ee4",
      "4615b3c5ce674070840fbb7bcb54f262",
      "b57f5b94be66435fab8e8aa9b877848c",
      "8b7c3bcf27644ac09871f733662c4d8b",
      "42cabc32ffc34ddd924fde45571ada38",
      "58c63ae0605a4f1192c52e4306602fd7",
      "55e0a986be424043846c0e863158a1bd",
      "23a9e406e89041f8ab9a15eae4f9c61b",
      "f18d3449a23c485a85e38f0ced810c99",
      "1d4352761751419282c93eb85c0954c0",
      "b84453116ba14690b8ae8a8e723f8510",
      "6e14135b00e447c18a0a693d47f5c92b",
      "9a540d6ed00a424da320fd7a12c31ac5",
      "13f07d79c96146aaa1d7731526aaec29",
      "b54312c4603b400b97884eba33ad1095",
      "b93ed8d01dbd4e0a84ff01b2e15f68ec",
      "8119e3c1840d4e0b975055cb0c255208",
      "ad785ec6b7924f2ebcdf0957a16c656d",
      "d44b6d406224497d9db5c903c6972323",
      "d3010a25994f4f12b95922f2863f4f3b"
     ]
    },
    "id": "Rt4ecMkaryYG",
    "outputId": "f8ab6e1a-2257-48bd-9fa6-c7fe19188404"
   },
   "outputs": [],
   "source": [
    "# load the Dense Prediction Transformer (DPT) model for getting normal maps\n",
    "depth_estimator = pipeline(\"depth-estimation\", model =\"Intel/dpt-hybrid-midas\")\n",
    "\n",
    "# load the controlnet model for normal maps\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"fusing/stable-diffusion-v1-5-controlnet-normal\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# load the stable diffusion pipeline with controlnet\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n",
    ").to(\"xpu\")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "id": "0WLxPD8fryYG",
    "outputId": "d605305f-4c8e-40dd-e238-13131f64c961"
   },
   "outputs": [],
   "source": [
    "image_input = load_image(\"https://cdn.pixabay.com/photo/2023/06/07/13/02/butterfly-8047187_1280.jpg\")\n",
    "image_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "Uqhw6NR4ryYG",
    "outputId": "2f3c8930-541b-49d7-fb87-374972c078e5"
   },
   "outputs": [],
   "source": [
    "# do all the preprocessing to get the normal image\n",
    "image = depth_estimator(image_input)['predicted_depth']\n",
    "\n",
    "image = image.numpy()\n",
    "\n",
    "image_depth = image.copy()\n",
    "image_depth -= np.min(image_depth)\n",
    "image_depth /= np.max(image_depth)\n",
    "\n",
    "bg_threhold = 0.4\n",
    "\n",
    "x = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\n",
    "x[image_depth < bg_threhold] = 0\n",
    "\n",
    "y = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\n",
    "y[image_depth < bg_threhold] = 0\n",
    "\n",
    "z = np.ones_like(x) * np.pi * 2.0\n",
    "\n",
    "image = np.stack([x, y, z], axis=2)\n",
    "image /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\n",
    "image = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\n",
    "image_normal = Image.fromarray(image)\n",
    "\n",
    "image_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433,
     "referenced_widgets": [
      "7aed2e247f5e488991be8b97ab6bae6a",
      "f8b042b72ea1403284bc45b55cbfcdcf",
      "9dcbc2386e3c4d6e820fd6baed91f0df",
      "67768915a19a448c94620204f861d005",
      "a218d578aded48ba8acca854622c14a9",
      "5cbe59686c0d475a8e2274ffcb64d992",
      "3bf3dd9f53354c758187674b5b1eac19",
      "a1ed5abc44e94c1ea19fa959fc95b91a",
      "7669fb4960904becb83cbdc4169a121b",
      "7699034aeb584f12864aaa30a7f2e6b5",
      "0762e7a5efb84a4b9bf60c5cf3235671"
     ]
    },
    "id": "_BLefbf1ryYG",
    "outputId": "be063e2c-4e78-4c1e-cbe3-98fd0dcdad72"
   },
   "outputs": [],
   "source": [
    "image_output = pipe(\"A colorful butterfly sitting on apples\", image_normal, num_inference_steps=20).images[0]\n",
    "image_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433,
     "referenced_widgets": [
      "ad63323795ae48679bb40de0056219d3",
      "4cb413851dc3469a8ea04e40373b11de",
      "21549aef7c6f4c64b8eb676c6dd105e2",
      "7cd3b5a0ce274ae5b5954a3c7d9d3bb4",
      "4a3a0d14e22f4a6e9dbb5fb35bc33e8d",
      "8d2f2b219308442db07d23b057f8e7cf",
      "7b9508943bee4f76b996ba561d4bbe9b",
      "94cc8073f13f45b085e1f23d3d4bbbc9",
      "cd502f2ef8814082979d147f08177ff6",
      "f357992a28754d7689721ebda28f0b0f",
      "2f8938d9a9254868869767a332bdf84f"
     ]
    },
    "id": "c-iOJPe1ryYH",
    "outputId": "ee338887-1753-47f0-aed3-838749415d3a"
   },
   "outputs": [],
   "source": [
    "image_output = pipe(\"A beautiful design\", image_normal, num_inference_steps=20).images[0]\n",
    "image_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zlU00SCzecq"
   },
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLEetydkzec_"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "referenced_widgets": [
      "cbbf80dd8dc9471197c49732ae4d08ee",
      "0a8a88ac007a41e4b0096fb114fe8a47",
      "d4c98d8a36eb4f769e23aa3f36535a0f",
      "20f315f7aae840d19ee8a068c68ff63c",
      "c420c12b2a76457c9ec7bb9db8eac9b3",
      "e38139b6898e4897b67aac89ee982c7a",
      "5e144bb789054a88a43db4329bb2df05",
      "15de6a583c5e4525a559749577fd4331",
      "4af5ce87c2a243238375d7edf93d08b2",
      "56cc25e58ea3445f9135817143224d2c",
      "6c6f19d716d2410b813e32f3b051619a",
      "7f1b3b31059e4abbbe62bd88da98627d",
      "0ee14dea902c4efe81c616f34dbc7562",
      "e43b78d4e4844565ad39e979861285a8",
      "9ba3483798e8439aab7d5560c1a5bac1",
      "993b867eff394cfe843eb18ee194b8d6",
      "b1659a9d5e294f9bbc1b20e892e16326",
      "308c9dc88e034f4bbcb4ee24628694ba",
      "0e38cf52fb3d40d5a87ec975499ef648",
      "a37c136cb6c34c1ea5a66bd78c5750ae",
      "60ebb0e8c1174c839a26182b38d4ae9b",
      "f3be39d3186448afb9d851b9732df52b",
      "7d9df7ed952144dfacc3d2908497ce94",
      "7010861902ab409e9a4f65efad33683c",
      "978ecf658d464bc999bc8a5c540e3ad0",
      "198e8a1603fc40218d805d46da1cc603",
      "ee7d2281e6964da8a3e610343a7de76d",
      "c30088b9866c429fa513397b03d2e3c8",
      "f48a988402a947a098ed14bfd0dc9a53",
      "f0a064bb91524f01972e941a3bd7354d",
      "7efd39ed94f94ecd9292069da0697a5f",
      "54746d0ef2e7453faddcc3b6066def51",
      "ff535db7c20f4719820b0b483a3a41ff",
      "3282a8068816447285333083b4c8fe96",
      "2c260ce7a89840e3857942d9294f9321",
      "eba07e4598814b6cb22ba58d16587cf3",
      "cfe9f9f05cb8493f804cd521a3e3c1c1",
      "b3418378805c4029aa6e6b939ae3c84b",
      "2cd604429546460d8e79a7ac4980d4cd",
      "1cf1b7e84b744edf907f94aa3d1983bc",
      "ab44913ac7c44a0f89a963fddf9dba39",
      "c9678e8106544428a9f155e2da9f2693",
      "15ce5867e9ba470b94f0408cfa8236ad",
      "ee848051813f401c958c169f8b77a323",
      "71ee3795a28a4776bdb13f68f11ffaaa",
      "af113f9b9b1749f0a856b3f371f03a2d",
      "83cb84140f504cdaa6d5ed308baaa9bf",
      "f81e6ba8c0ee4d19afc1691be3650a94",
      "8bc53de7028e4a05bc683a6f47548d62",
      "72fc016402374ad996a292470fa75906",
      "4ec20db88a97441982a914517888a8b4",
      "75837fa1f8ed4a7b8734f0c49e3bb6d0",
      "527478832a8e451ea056c50ee2b241ab",
      "e1877e0c82a1464597250ef0572d2679",
      "3005ddef183645cbb3039da2a621ebf4"
     ]
    },
    "id": "BRZqE7YLzedA",
    "outputId": "b9f4d2e9-2667-4992-f3d5-f574108640ec"
   },
   "outputs": [],
   "source": [
    "# load the image processor and the model for doing segmentation\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-small\")\n",
    "image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-small\")\n",
    "\n",
    "# load the controlnet model for semantic segmentation\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-seg\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# load the stable diffusion pipeline with controlnet\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n",
    ").to(\"xpu\")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kZI0eD2zkfm"
   },
   "outputs": [],
   "source": [
    "# define color palette that is used by the semantic segmentation models\n",
    "\n",
    "palette = np.asarray([\n",
    "    [0, 0, 0],\n",
    "    [120, 120, 120],\n",
    "    [180, 120, 120],\n",
    "    [6, 230, 230],\n",
    "    [80, 50, 50],\n",
    "    [4, 200, 3],\n",
    "    [120, 120, 80],\n",
    "    [140, 140, 140],\n",
    "    [204, 5, 255],\n",
    "    [230, 230, 230],\n",
    "    [4, 250, 7],\n",
    "    [224, 5, 255],\n",
    "    [235, 255, 7],\n",
    "    [150, 5, 61],\n",
    "    [120, 120, 70],\n",
    "    [8, 255, 51],\n",
    "    [255, 6, 82],\n",
    "    [143, 255, 140],\n",
    "    [204, 255, 4],\n",
    "    [255, 51, 7],\n",
    "    [204, 70, 3],\n",
    "    [0, 102, 200],\n",
    "    [61, 230, 250],\n",
    "    [255, 6, 51],\n",
    "    [11, 102, 255],\n",
    "    [255, 7, 71],\n",
    "    [255, 9, 224],\n",
    "    [9, 7, 230],\n",
    "    [220, 220, 220],\n",
    "    [255, 9, 92],\n",
    "    [112, 9, 255],\n",
    "    [8, 255, 214],\n",
    "    [7, 255, 224],\n",
    "    [255, 184, 6],\n",
    "    [10, 255, 71],\n",
    "    [255, 41, 10],\n",
    "    [7, 255, 255],\n",
    "    [224, 255, 8],\n",
    "    [102, 8, 255],\n",
    "    [255, 61, 6],\n",
    "    [255, 194, 7],\n",
    "    [255, 122, 8],\n",
    "    [0, 255, 20],\n",
    "    [255, 8, 41],\n",
    "    [255, 5, 153],\n",
    "    [6, 51, 255],\n",
    "    [235, 12, 255],\n",
    "    [160, 150, 20],\n",
    "    [0, 163, 255],\n",
    "    [140, 140, 140],\n",
    "    [250, 10, 15],\n",
    "    [20, 255, 0],\n",
    "    [31, 255, 0],\n",
    "    [255, 31, 0],\n",
    "    [255, 224, 0],\n",
    "    [153, 255, 0],\n",
    "    [0, 0, 255],\n",
    "    [255, 71, 0],\n",
    "    [0, 235, 255],\n",
    "    [0, 173, 255],\n",
    "    [31, 0, 255],\n",
    "    [11, 200, 200],\n",
    "    [255, 82, 0],\n",
    "    [0, 255, 245],\n",
    "    [0, 61, 255],\n",
    "    [0, 255, 112],\n",
    "    [0, 255, 133],\n",
    "    [255, 0, 0],\n",
    "    [255, 163, 0],\n",
    "    [255, 102, 0],\n",
    "    [194, 255, 0],\n",
    "    [0, 143, 255],\n",
    "    [51, 255, 0],\n",
    "    [0, 82, 255],\n",
    "    [0, 255, 41],\n",
    "    [0, 255, 173],\n",
    "    [10, 0, 255],\n",
    "    [173, 255, 0],\n",
    "    [0, 255, 153],\n",
    "    [255, 92, 0],\n",
    "    [255, 0, 255],\n",
    "    [255, 0, 245],\n",
    "    [255, 0, 102],\n",
    "    [255, 173, 0],\n",
    "    [255, 0, 20],\n",
    "    [255, 184, 184],\n",
    "    [0, 31, 255],\n",
    "    [0, 255, 61],\n",
    "    [0, 71, 255],\n",
    "    [255, 0, 204],\n",
    "    [0, 255, 194],\n",
    "    [0, 255, 82],\n",
    "    [0, 10, 255],\n",
    "    [0, 112, 255],\n",
    "    [51, 0, 255],\n",
    "    [0, 194, 255],\n",
    "    [0, 122, 255],\n",
    "    [0, 255, 163],\n",
    "    [255, 153, 0],\n",
    "    [0, 255, 10],\n",
    "    [255, 112, 0],\n",
    "    [143, 255, 0],\n",
    "    [82, 0, 255],\n",
    "    [163, 255, 0],\n",
    "    [255, 235, 0],\n",
    "    [8, 184, 170],\n",
    "    [133, 0, 255],\n",
    "    [0, 255, 92],\n",
    "    [184, 0, 255],\n",
    "    [255, 0, 31],\n",
    "    [0, 184, 255],\n",
    "    [0, 214, 255],\n",
    "    [255, 0, 112],\n",
    "    [92, 255, 0],\n",
    "    [0, 224, 255],\n",
    "    [112, 224, 255],\n",
    "    [70, 184, 160],\n",
    "    [163, 0, 255],\n",
    "    [153, 0, 255],\n",
    "    [71, 255, 0],\n",
    "    [255, 0, 163],\n",
    "    [255, 204, 0],\n",
    "    [255, 0, 143],\n",
    "    [0, 255, 235],\n",
    "    [133, 255, 0],\n",
    "    [255, 0, 235],\n",
    "    [245, 0, 255],\n",
    "    [255, 0, 122],\n",
    "    [255, 245, 0],\n",
    "    [10, 190, 212],\n",
    "    [214, 255, 0],\n",
    "    [0, 204, 255],\n",
    "    [20, 0, 255],\n",
    "    [255, 255, 0],\n",
    "    [0, 153, 255],\n",
    "    [0, 41, 255],\n",
    "    [0, 255, 204],\n",
    "    [41, 0, 255],\n",
    "    [41, 255, 0],\n",
    "    [173, 0, 255],\n",
    "    [0, 245, 255],\n",
    "    [71, 0, 255],\n",
    "    [122, 0, 255],\n",
    "    [0, 255, 184],\n",
    "    [0, 92, 255],\n",
    "    [184, 255, 0],\n",
    "    [0, 133, 255],\n",
    "    [255, 214, 0],\n",
    "    [25, 194, 194],\n",
    "    [102, 255, 0],\n",
    "    [92, 0, 255],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "id": "GjsCwREqzedB",
    "outputId": "6ac8fcaf-a89b-4bef-bd9b-31fd753568d7"
   },
   "outputs": [],
   "source": [
    "image_input = load_image(\"https://cdn.pixabay.com/photo/2023/02/24/07/14/crowd-7810353_1280.jpg\")\n",
    "image_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "id": "qK5gkfCIzedB",
    "outputId": "71e92874-04d1-459a-8cc6-89bc31796b2f"
   },
   "outputs": [],
   "source": [
    "# get the pixel values\n",
    "pixel_values = image_processor(image_input, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# do semantic segmentation\n",
    "with torch.no_grad():\n",
    "  outputs = image_segmentor(pixel_values)\n",
    "\n",
    "# post process the semantic segmentation\n",
    "seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image_input.size[::-1]])[0]\n",
    "\n",
    "# add colors to the different identified classes\n",
    "color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[seg == label, :] = color\n",
    "\n",
    "# convert into PIL image format\n",
    "color_seg = color_seg.astype(np.uint8)\n",
    "image_seg = Image.fromarray(color_seg)\n",
    "\n",
    "image_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897,
     "referenced_widgets": [
      "09441b43551f4e5f9ff77512f59d2f9e",
      "955af4f359254125801ec776793397e7",
      "27f8dc0ef76448be90b004529fa562cf",
      "a3f598a48fa448dbb44a40c8c7af85e6",
      "6c353682d9d64022a11f31357816f14d",
      "8c0d69533b674629b69a64345fc4d67e",
      "f2241131e8d54c1f9e8976b94b46bc67",
      "0120420c938f4e588ac496f37bcc42eb",
      "89a159fc68ef4972a0368c72306cd8c4",
      "c8ce8fd6e04e416381d892fbf75a0b75",
      "8d62ab081ff94e7abe3cc911d3376bbd"
     ]
    },
    "id": "z0eWMTU0zedB",
    "outputId": "9502f927-18cb-4baa-dc24-9c18ec4ce150"
   },
   "outputs": [],
   "source": [
    "image_output = pipe(\"A crowd of people staring at a glorious painting\", image_seg, num_inference_steps=20).images[0]\n",
    "image_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897,
     "referenced_widgets": [
      "312f1ce28d144f8e828d5d33c0783a58",
      "36aae82533a5470abfa287df328448fc",
      "7afd5032c0964fc5823189e2fc8599ed",
      "944390d866cf430fbd562fca7781a9d9",
      "66f7c6f267864c929727fd96a83e6d06",
      "2e3313ac79794a18a38a6b98c0f4f28a",
      "77ad5edb54bb4e30bdd82186e80367c8",
      "c9311122d93c495faa2a8e0f4b8f7662",
      "58657e2570434d73be122eafb42f490a",
      "ef977ff25ba842c9a0475befc7b47b89",
      "c8da6be1960c4f89a18d49205cbaa0df"
     ]
    },
    "id": "LbWbrzpszedB",
    "outputId": "4bdd8997-e047-4fbb-ef58-6162ad40a4be"
   },
   "outputs": [],
   "source": [
    "image_output = pipe(\"Aliens looking at earth from inside their spaceship from a window, not creepy, not scary, not gross, octane render, smooth\", image_seg, num_inference_steps=20).images[0]\n",
    "image_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXmBKD8G7WRU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "h6XPbq0FrPQG",
    "HEiobo68Kzso",
    "fwnnyHq3oi7O",
    "-o-9jxdtpvgi",
    "-uc3OBwGryX9"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch 2.6",
   "language": "python",
   "name": "pytorch-2.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
