{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfsDR_omdNea"
   },
   "source": [
    "# LLM Basics with Hugging Face\n",
    "This notebook demonstrates how to load LLM models by utilizing Hugging Face, and how to make queries.\n",
    "<!--table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/Gemma_Basics_with_HF.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Adapted for EECE.4860/5860 at UMass Lowell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaqZItBdeokU"
   },
   "source": [
    "## Prerequisites \n",
    "\n",
    "### Account on Intel Tiber AI Cloud (or run on your local GPU if available)\n",
    "\n",
    "You will need a standard account on Intel Tiber AI Cloud, where we have tested this notebook. Students have been given instructions on how to sign up for an account on Intel Tiber.\n",
    "\n",
    "### HuggingFace setup\n",
    "\n",
    "Before we dive into the tutorial, let's get you set up with HuggingFace:\n",
    "\n",
    "1. **Hugging Face Account:**  If you don't already have one, you can create a free Hugging Face account by clicking [here](https://huggingface.co/join).\n",
    "2. **LLM Model Access:** Head over to the [Gemma model page](https://huggingface.co/google/gemma-2b) and [llama2 model papge](https://huggingface.co/meta-llama/Llama-2-7b-hf) and accept the usage conditions.\n",
    "3. **Hugging Face Token:**  You need to create a token on HuggingFace and use it to login from this notebook. Once you are logged in, you can download the models. Check [this guide](https://huggingface.co/docs/hub/en/security-tokens) on how to create a token on HF. Generate a Hugging Face access (preferably `write` permission) token by clicking [here](https://huggingface.co/settings/tokens). **Save the token in a safe document that you can access**. Once you've completed these steps, you're ready to move on to the next section where we'll install necessary packages and log into HuggingFace Hub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFLddpGeaKh5"
   },
   "source": [
    "**If there is no error in the previous step, you are all set and ready to explore the possibilities with LLM models!**\n",
    "\n",
    "\n",
    "**You need to click the next cell to proceed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXFZFUJHgTcU"
   },
   "source": [
    "## Instantiate the Gemma 2B model (or other models)\n",
    "\n",
    "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n",
    "\n",
    "Please note we list here a few variants of the Gemma models for you to play with.\n",
    "\n",
    "Other models is this example include Llama 2 from Meta.\n",
    "\n",
    "Let's get started by loading the model from Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_HOME=/opt/notebooks/.cache/huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_bahJBmwvSp"
   },
   "source": [
    "### Log into Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIFFCHi-wvSp"
   },
   "outputs": [],
   "source": [
    "# you could use OS env variable to store the HF token\n",
    "#from huggingface_hub import login\n",
    "#login(os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# or use an input box on this notebook to copy/paste the token\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgl8ZjHpwvSq"
   },
   "source": [
    "### Loading the model from HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_z4600bwvSq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74tpQWWWwvSq"
   },
   "outputs": [],
   "source": [
    "# Let's load the tokenizer first\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UD-eXTxxwvSq"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# We could typically quantize the model to reduce its weight\n",
    "# But to simplify the process, we won't quantize it in this notebook\n",
    "\n",
    "# Let's load the chosen model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lyw7fwOGwvSq"
   },
   "source": [
    "### Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrVBVTtlwvSq"
   },
   "outputs": [],
   "source": [
    "prompt = \"My favourite color is\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=20)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrVBVTtlwvSq"
   },
   "outputs": [],
   "source": [
    "prompt = \"Who won the 2016 baseball World Series? Answer:\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=40)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYT6m2LNvxdo"
   },
   "outputs": [],
   "source": [
    "prompt = \"What can you use an LLM for? Answer:\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Gemma_Basics_with_HF.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
