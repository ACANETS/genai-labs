{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfsDR_omdNea"
   },
   "source": [
    "# Lab 1 - EECE.4860/5860 at UMass Lowell - Spring 2025\n",
    "\n",
    "This lab requires you to practice with LLMs on Intel Tiber AI cloud, such as how to load LLM models by utilizing Hugging Face, change model settings, make queries, and generate required outputs through prompt engineering. Students in EECE.5860 will practice how to finetune an LLM using a new dataset.\n",
    "\n",
    "## **Note** This lab has three parts. Part 1 and Part 2 are for all the students. Part 3 contains additional requirements for EECE.5860 students. EECE.4860 students do not need to complete Part 3.\n",
    "\n",
    "\n",
    "## Part 1:   Simple LLM Inference (10 points)\n",
    "\n",
    "Execute the example notebook [``Simple LLM Inference``](simple_llm_inference.ipynb) to experiment with different models and their settings (e.g. temperature). Record the differences in their outputs when given the same input prompt. Evaluate the quality of their completions using your own words. (10 points)\n",
    "\n",
    "### Questions: \n",
    "\n",
    "(1) What are the parameters you could configure for an LLM in this example notebook?\n",
    "\n",
    "(2) Pick one parameter, discuss the impact of changing its value on the model's completions.\n",
    "\n",
    "## Part 2: Prompt Engineering (10 points)\n",
    "\n",
    "Use **gemma-2-2b-it** model and **one** prompt to generate the list of winners and years of the World Series Championships from 2010 to **2025**. You can use the example notebook [``LLM_Basics_with_HF.ipynb``](LLM_Basics_with_HF.ipynb) as a starting point. Your prompt and completion will be graded using the following rubrics:\n",
    "\n",
    "* the output should include a list of winners and years in json format. (6 points)\n",
    "* the json output must include the year of 2024 (2 points)\n",
    "* the output needs to discuss/explain the information about the year of 2025 (2 points)\n",
    "\n",
    "### Questions: \n",
    "\n",
    "(1) What years of the World Series Championship results are already known to the model without any context learning? Why?\n",
    "\n",
    "(2) When a model is loaded using transformers library (e.g., ``AutoModelForCausalLM.from_pretrained(model_id)`` ) from HuggingFace, what exact files are downloaded? \n",
    "\n",
    "## Part 3: Model Fine Tuning (10 points)\n",
    "\n",
    "Building on the example notebook [``gemma_xpu_finetuning.ipynb``](gemma_xpu_finetuning.ipynb), you will use **gemma-2-2b-it** model and a new dataset [``lavita/ChatDoctor-HealthCareMagic-100k``](https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k) to finetune the base **gemma-2-2b-it** model. Your notebook will be graded as follows:\n",
    "\n",
    "* You first sample a few questions from the dataset as prompts to assess the quality of the completions from the original gemma-2-2b-it model. (3 points)\n",
    "* Then you will use this dataset to finetune the base model and save it as a new model called **gemma-2-2b-it-finetuned**. (4 points)\n",
    "* Query the finetuned model using the same test prompts and compare its completions with the base model. (3 points)\n",
    "\n",
    "### Questions:\n",
    "\n",
    "(1) How do you prepare training and validate data samples and preprocess them according to a prompt template?\n",
    "\n",
    "(2) How long does it take for the model to finetune its parameters using the dataset? Discuss how some of the training parameters can affect the training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Submission\n",
    "\n",
    "Please use screenshots to document the outputs from your notebooks. Include the screenshots in your lab report, which is due by the posted due dates."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Gemma_Basics_with_HF.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PyTorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
