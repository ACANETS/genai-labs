{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfsDR_omdNea"
   },
   "source": [
    "# Lab 2 - EECE.4860/5860 at UMass Lowell - Spring 2025\n",
    "\n",
    "This lab is to practice the usage of LoRA to fine-tune a pretrained model, perform quantitative evaluation of model performance, and experiment with Reinforcement Learning with Human Feedback (RLHF) (for EECE.5860 students). You will run Jupyter notebooks on Intel Tiber AI cloud. \n",
    "\n",
    "## **Note** There are two parts in this lab. Parts 1 is for all the students. Part 2 is an additional requirement intended for EECE.5860 students only. There are some questions for each part, and you need to answer them in your lab report.\n",
    "\n",
    "\n",
    "## Part 1:   LLoRA Fine-Tuning and Quantitative Evaluation of Models (10 points)\n",
    "\n",
    "Complete the [Lab2Part1-LoRA-ROUGE.ipynb](Lab2Part1-LoRA-ROUGE.ipynb) to experiment with LoRA fine-tuning. There are missing codes in the given notebook, and you will add the missing code by following the instructions in the notebook. You goal is to compare the performance of the fine-tuned model with the original model. Evaluate the models using quantitative metrics such as ROUGE scores. Your notebook will be graded as follows:\n",
    "\n",
    "* performs LoRA fine-tuning successfully (6 points)\n",
    "* calculates the aggregated ROUGE scores (rouge1, rouge2, rougeL, rougeLsum) using 15 rows of test data. That is, you will use the summaries generated for the 15 prompts from the test dataset, and their corresponding 15 reference summaries, to compute the aggregated ROUGE scores (4 points)\n",
    "\n",
    "## Part 2: Model Fine-Tuning with RLHF and PPO (10 points)\n",
    "\n",
    "Complete the notebook [Lab2Part2-RLHF-PPO.ipynb](Lab2Part2-RLHF-PPO.ipynb) to experiment with RLHF and PPO. You will implement the fine-tuning of the **google/flan-t5-base** model using the **knkarthick/dialogsum** dataset. Then you will implement RLHF fine-tuning using a PPOTrainer to reduce the toxicity of the fine-tuned model. Specifically, you will need to use the toxicity model as the reward model, and a copy of the fine-tuned model as the frozen reference model. Follow the instructions in the notebook to complete the missing source codes. We will grade your notebook as follows:\n",
    "* You fine-tune the base model and save it for later RLHF based fine-tuning. (4 points)\n",
    "* You use PPOTrainer to implement RLHF based fine-tuning to detoxify the fine-tuned model. (3 points)\n",
    "* You show the improvement of the fine-tuned model by computing the mean and standard deviation of Toxicity scores of before and after. (3 points)\n",
    "\n",
    "\n",
    "# Lab submission \n",
    "\n",
    "You need to follow the lab report guidelines (on blackboard) for report writing. Submit your lab report on Blackboard by the posted deadline.\n",
    "\n",
    "You will need to manage your source code on github.com under your own private repository. Do not copy/paste complete source code in your lab report. Instead include in the report a link to your github repo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Gemma_Basics_with_HF.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PyTorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
