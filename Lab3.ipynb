{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfsDR_omdNea"
   },
   "source": [
    "# Lab 3 - EECE.4860/5860 at UMass Lowell - Summer 2025\n",
    "\n",
    "This lab is to practice the usage of Retrieval Augmented Generation (RAG) to understand how to enhance a LLM to obtain new knowledge without retraining. EECE.5860 students will also experiment with AI agent using LangGraph. You will run Jupyter notebooks on Intel Tiber AI cloud. \n",
    "\n",
    "## **Note** There are two parts in this lab. Parts 1 is for all the students. Part 2 is an additional requirement intended for EECE.5860 students only. There are some questions for each part, and you need to answer them in your lab report.\n",
    "\n",
    "\n",
    "## Part 1:   RAG (10 points)\n",
    "\n",
    "Retrieval Augmented Generation (RAG)  merges the powerful retrieval capabilities of large-scale search systems with the generative strengths of transformer models. In the given “simple_rag.ipynb” notebook, the system first retrieves relevant documents or passages from a corpus in response to a query. These retrieved materials are then combined with the query and input into the language model. This two-stage process allows the model to draw on both external information from the corpus and its internal knowledge, resulting in more accurate and contextually rich responses.\n",
    "\n",
    "First execute the [simple_rag.ipynb](simple_rag.ipynb) notebook to experiment with the RAG process. Choose a few different settings for response customization (e.g., top K), dataset (e.g., chunk size), etc., and compare the responses qualitatively. Then you will need to modify the code to add a different LLM such as *google/gemma-2-2b-it*. Your part 1 of the lab will be graded as follows:\n",
    "\n",
    "* Execute the given implementation successfully. Take screenshots to demonstrate the results (4 points)\n",
    "* Modify the code to add a different LLM model for user to choose (3 points)\n",
    "* Answer the questions related to this implementation. (3 point)\n",
    "\n",
    "### Questions: \n",
    "\n",
    "(1) What are the APIs used for chunking the documents? What arguments were used ?\n",
    "\n",
    "(2) What would be the impact if you increase the parameter \"top_k\"? \n",
    "\n",
    "(3) What text splitter API should you choose if your documents are Python programs? [API Reference](https://python.langchain.com/api_reference/text_splitters/index.html)\n",
    "\n",
    "\n",
    "## Part 2: AI agent using LangGraph based API (15 points)\n",
    "\n",
    "Complete the notebook [Lab3Part2-langgraph-agent.ipynb](Lab3Part2-langgraph-agent.ipynb) to experiment with AI agent using LangGraph and ChatNVIDIA chat model. Specifically, you will use a LangGraph agent to process and respond to user queries through a language model interface. It sets up a conversational agent by integrating LLM with other LangChain tools, enabling the agent to perform tasks such as RAG query and calculation. You will need to write a new RAG tool, and supply it to the agent. You can follow the [simple_rag.ipynb](simple_rag.ipynb) example to implement the RAG tool. Your Part 2 of the lab will be graded as follows:\n",
    "\n",
    "* Your completion of the new RAG tool and use of LangGraph APIs to complete the queries. Take screenshots to demonstrate the results (10 points)\n",
    "* Answer the questions related to this implementation. (5 point)\n",
    "\n",
    "### Questions: \n",
    "\n",
    "(1) What are \"chat models\" for LangGraph tools? And why do we use ChatNVIDIA model in our example?\n",
    "\n",
    "(2) What are the functions of the tools included in the python list named \"tools\"? If you remove the \"wikipedia_tool\", what result would you likely get and why?\n",
    "\n",
    "(3) What are the essential steps of creating a LangGraph agent and execute the agent?\n",
    "\n",
    "# Lab submission \n",
    "\n",
    "You need to follow the lab report guidelines (on blackboard) for report writing. Submit your lab report on Blackboard by the posted deadline.\n",
    "\n",
    "You will need to manage your source code on github.com under your own private repository. Do not copy/paste complete source code in your lab report. Instead include in the report a link to your github repo.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Gemma_Basics_with_HF.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PyTorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
