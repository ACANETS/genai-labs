{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfsDR_omdNea"
   },
   "source": [
    "# Toxicity Detector by Meta\n",
    "This notebook demonstrates how to use Meta toxicity model.\n",
    "\n",
    "Some parts of the codes are from the original source: https://github.com/aws-samples/data-science-on-aws/blob/main/05b_RLHF_Fine_Tune_Model_to_Detoxify_Summaries.ipynb\n",
    "<!--table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/Gemma_Basics_with_HF.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Adapted for EECE.4860/5860 at UMass Lowell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaqZItBdeokU"
   },
   "source": [
    "## Prerequisites \n",
    "\n",
    "### Account on Intel Tiber AI Cloud (or run on your local GPU if available)\n",
    "\n",
    "You will need a standard account on Intel Tiber AI Cloud, where we have tested this notebook. Students have been given instructions on how to sign up for an account on Intel Tiber.\n",
    "\n",
    "Once you open this notebook on Tiber cloud, make sure to select **PyTorch 2.7** kernel to run it.\n",
    "\n",
    "### HuggingFace setup\n",
    "\n",
    "Before we dive into the tutorial, let's get you set up with HuggingFace:\n",
    "\n",
    "1. **Hugging Face Account:**  If you don't already have one, you can create a free Hugging Face account by clicking [here](https://huggingface.co/join).\n",
    "2. **LLM Model Access:** Head over to the [Gemma model page](https://huggingface.co/google/gemma-2b) and [llama2 model papge](https://huggingface.co/meta-llama/Llama-2-7b-hf) and accept the usage conditions.\n",
    "3. **Hugging Face Token:**  You need to create a token on HuggingFace and use it to login from this notebook. Once you are logged in, you can download the models. Check [this guide](https://huggingface.co/docs/hub/en/security-tokens) on how to create a token on HF. Generate a Hugging Face access (preferably `write` permission) token by clicking [here](https://huggingface.co/settings/tokens). **Save the token in a safe document that you can access**. Once you've completed these steps, you're ready to move on to the next section where we'll install necessary packages and log into HuggingFace Hub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwjo5_Uucxkw"
   },
   "source": [
    "### Import Necessary Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9sUQ4WrP-Yr"
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwjo5_Uucxkw"
   },
   "source": [
    "### Install dependencies\n",
    "Run the cell below to install all the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_nXPEsF7UWQ"
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install --upgrade -q transformers huggingface_hub peft \\\n",
    "  accelerate bitsandbytes datasets trl ipywidgets evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_bahJBmwvSp"
   },
   "source": [
    "### Log into Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIFFCHi-wvSp"
   },
   "outputs": [],
   "source": [
    "# you could use OS env variable to store the HF token\n",
    "#from huggingface_hub import login\n",
    "#login(os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# or use an input box on this notebook to copy/paste the token\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFLddpGeaKh5"
   },
   "source": [
    "**If there is no error in the previous step, you are all set and ready to explore the possibilities with LLM models!**\n",
    "\n",
    "\n",
    "**You need to click the next cell to proceed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXFZFUJHgTcU"
   },
   "source": [
    "## Instantiate the Roberta-hate-speech-dynabench-r4-target model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_HOME=/opt/notebooks/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_z4600bwvSq"
   },
   "outputs": [],
   "source": [
    "#device = \"cuda\"\n",
    "USE_CPU = False\n",
    "device = \"xpu:0\" if torch.xpu.is_available() else \"cpu\"\n",
    "if USE_CPU:\n",
    "    device = \"cpu\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgl8ZjHpwvSq"
   },
   "source": [
    "### Loading the model from HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74tpQWWWwvSq"
   },
   "outputs": [],
   "source": [
    "# Let's load the tokenizer and model \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgl8ZjHpwvSq"
   },
   "source": [
    "### Evaluate the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UD-eXTxxwvSq"
   },
   "outputs": [],
   "source": [
    "non_toxic_text = \"You are a great person and I like you.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# get the logits for \"not hate\" - this is the reward!\n",
    "not_hate_index = 0\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (high): {nothate_reward}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_text = \"You are disgusting and terrible and i damn hate you\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "logits = toxicity_model(toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# Get the logits for \"not hate\" - this is the reward!\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist() \n",
    "print(f'reward (low): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lyw7fwOGwvSq"
   },
   "source": [
    "### Evaluate the Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrVBVTtlwvSq"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "toxicity_evaluator = evaluate.load(\"toxicity\", \n",
    "                                    toxicity_model_name,\n",
    "                                    module_type=\"measurement\",\n",
    "                                    toxic_label=\"hate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrVBVTtlwvSq"
   },
   "outputs": [],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    non_toxic_text\n",
    "])\n",
    "\n",
    "print(\"Toxicity score for non-toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])\n",
    "\n",
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    toxic_text\n",
    "])\n",
    "\n",
    "print(\"\\nToxicity score for toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Gemma_Basics_with_HF.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PyTorch 2.7",
   "language": "python",
   "name": "pytorch27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
