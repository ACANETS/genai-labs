{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0\n",
    "Copyright (c) 2025, Rahul Unnikrishnan Nair <rahul.unnikrishnan.nair@intel.com>\n",
    "Copyright (c) 2023, Eduardo Alvarez <eduardo.a.alvarez@intel.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) with Hugging Face Transformers on Intel® Data Center GPU\n",
    "\n",
    "**Note:** Please select the \"PyTorch 2.7\" kernel when running this notebook\n",
    "\n",
    "#### 1. Retrieval Augmented Generation (RAG):\n",
    "Retrieval Augmented Generation (RAG) is a novel approach that combines the strengths of large-scale retrieval systems with the generative capabilities of transformer models like Falcon. In our RAG-based system, implemented on Intel® Data Center GPU Max Series 1100, when a question is posed, relevant documents or passages are retrieved from a corpus, and then fed alongside the query to the language model. This two-step process enables the model to leverage both external knowledge from the corpus and its internal knowledge to produce more informed and contextually accurate responses.\n",
    "\n",
    "#### 2. In-context Learning:\n",
    "Traditional machine learning models learn from extensive labeled datasets. In contrast, in-context learning pertains to models, especially language models, leveraging a few examples or context provided at inference time to tailor their outputs. Our implementation with Falcon3-1B-Instruct demonstrates this capability through efficient context processing using the Transformers pipeline.\n",
    "\n",
    "#### 3. LLM Chains/Pipelines:\n",
    "LLM chains or pipelines involve stringing together multiple stages or components of a system to achieve a complex task. In our RAG system, the pipeline includes a vector database for efficient retrieval, followed by the Hugging Face Transformers pipeline for response generation. This modular approach allows for easy optimization and updates to individual components.\n",
    "\n",
    "#### 4. RAG for On-Premise LLM Applications:\n",
    "With the growing need for data privacy and proprietary data handling, many enterprises seek solutions to harness the power of LLMs in-house. Our implementation, leveraging Intel's GPU infrastructure, demonstrates how RAG can be deployed effectively on-premise. By integrating RAG with local data repositories and utilizing hardware acceleration, enterprises can build powerful LLM applications tailored to their specific needs while ensuring data confidentiality.\n",
    "\n",
    "#### 5. RAG vs Fine-Tuning:\n",
    "While RAG is a powerful approach on its own, it can also be combined with different model architectures to enhance LLM capabilities. Our implementation focuses on using RAG with the Falcon3-1B-Instruct model, demonstrating how external knowledge retrieval can complement the model's built-in capabilities. This approach allows for dynamic knowledge integration without the need for continuous model retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "1. Run the installation cell (first time only)\n",
    "2. Execute all cells in order\n",
    "3. The interactive interface will appear in the final cell\n",
    "4. Select your preferences and start chatting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Install dependencies. Only run the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "%pip install langchain accelerate transformers datasets tiktoken chromadb sentence_transformers langchain-community ipywidgets --no-warn-script-location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "The code below builds RAGBot. RAGBot class is a streamlined implementation of a Retrieval-Augmented Generation (RAG) chatbot, designed to integrate large language models for generating contextually relevant responses. At its core, it manages the downloading and initialization of HuggingFace Transformer models, such as the \"Falcon3-1B-Instruct\" model, with support for handling large model files. Running on Intel® Data Center GPU Max Series 1100, the system leverages hardware acceleration for efficient model inference. It automates the process of fetching and structuring dialogue datasets from predefined sources. The chatbot utilizes the HuggingFace Transformers library, allowing for efficient model inference with adjustable parameters like the number of threads and maximum tokens. A key feature of RAGBot is its ability to build a vector database for text retrieval, significantly bolstering its ability to pull relevant document snippets based on user queries. This functionality, combined with a retrieval mechanism and an inference method using the Transformers pipeline, makes RAGBot a simple tool for developers aiming to learn about RAG and leverage this implementation as a basis for their implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Package Imports\n",
    "\n",
    "Core libraries for RAG implementation including Hugging FaceTransformers, LangChain components, and utilities for UI and data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_HOME=/opt/notebooks/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import contextlib\n",
    "import pandas as pd\n",
    "import time\n",
    "import io\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### RAGBot Class Implementation\n",
    "\n",
    "The RAGBot class implements a Retrieval-Augmented Generation system optimized for Intel® Data Center GPU Max Series 1100. Key components include:\n",
    "\n",
    "#### Core Features\n",
    "- Model Management: Handles initialization of Hugging Face Transformer models (currently supporting Falcon3-1B-Instruct)\n",
    "- Dataset Handling: Manages dialogue datasets from various domains (robot maintenance, sports coaching, academia, retail)\n",
    "- Vector Database: Creates and manages embeddings for efficient text retrieval\n",
    "- RAG Pipeline: Combines context retrieval with language model inference\n",
    "\n",
    "#### Key Methods\n",
    "- `get_model()`: Initializes the specified language model\n",
    "- `download_dataset()`: Fetches and processes dialogue datasets\n",
    "- `load_model()`: Configures the transformer model with specified parameters\n",
    "- `build_vectordb()`: Creates a searchable vector database from text data\n",
    "- `retrieval_mechanism()`: Implements context retrieval based on user queries\n",
    "- `inference()`: Generates responses using the loaded model and retrieved context\n",
    "\n",
    "The implementation leverages Hugging Face Transformers for model operations and LangChain for RAG functionality, providing a streamlined approach to context-aware response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGBot:\n",
    "    \"\"\"\n",
    "    A class to handle model downloading, dataset management, model loading, vector database\n",
    "    creation, retrieval mechanisms, and inference for a response generation bot.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.model_name = \"\"\n",
    "        self.data_path = \"\"\n",
    "        self.user_input = \"\"\n",
    "        self.model = \"\"\n",
    "        self.max_tokens = 50\n",
    "        self.top_k = 50\n",
    "\n",
    "    def get_model(self, model, chunk_size: int = 10000):\n",
    "        self.model = model\n",
    "        if self.model == \"Falcon\":\n",
    "            self.model_name = \"tiiuae/Falcon3-1B-Instruct\"\n",
    "        elif model == \"More Models Coming Soon!\":\n",
    "            print(\"More models coming soon, defaulting to Falcon for now!\")\n",
    "            self.model_name = \"tiiuae/Falcon3-1B-Instruct\"\n",
    "\n",
    "    def download_dataset(self, dataset):\n",
    "        self.data_path = dataset + '_dialogues.txt'\n",
    "        if not os.path.isfile(self.data_path):\n",
    "            datasets = {\n",
    "                \"robot maintenance\": \"FunDialogues/customer-service-robot-support\", \n",
    "                \"basketball coach\": \"FunDialogues/sports-basketball-coach\", \n",
    "                \"physics professor\": \"FunDialogues/academia-physics-office-hours\",\n",
    "                \"grocery cashier\" : \"FunDialogues/customer-service-grocery-cashier\"\n",
    "            }\n",
    "            dataset = load_dataset(f\"{datasets[dataset]}\")\n",
    "            dialogues = dataset['train']\n",
    "            df = pd.DataFrame(dialogues, columns=['id', 'description', 'dialogue'])\n",
    "            dialog_df = df['dialogue']\n",
    "            dialog_df.to_csv(self.data_path, sep=' ', index=False)\n",
    "        else:\n",
    "            print('data already exists in path.')        \n",
    "\n",
    "    def load_model(self, n_threads, max_tokens, repeat_penalty, n_batch, top_k):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.llm.config.pad_token_id = self.llm.config.eos_token_id\n",
    "        self.max_tokens = max_tokens\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def build_vectordb(self, chunk_size, overlap):\n",
    "        loader = TextLoader(self.data_path)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "        self.index = VectorstoreIndexCreator(\n",
    "            embedding=HuggingFaceEmbeddings(), \n",
    "            text_splitter=text_splitter\n",
    "        ).from_loaders([loader])\n",
    "\n",
    "    def retrieval_mechanism(self, user_input, top_k=1, context_verbosity=False, rag_off=False):\n",
    "        self.user_input = user_input\n",
    "        self.context_verbosity = context_verbosity   \n",
    "        results = self.index.vectorstore.similarity_search(self.user_input, k=top_k)\n",
    "        context = \"\\n\".join([document.page_content for document in results])\n",
    "        if self.context_verbosity:\n",
    "            print(f\"Retrieving information related to your question...\")\n",
    "            print(f\"Found this content which is most similar to your question: {context}\")\n",
    "        if rag_off:\n",
    "            self.full_prompt = f\"Question: {user_input}\\nAnswer:\"\n",
    "        else:     \n",
    "            self.full_prompt = f\"\"\"Context: {context}\\n\\nQuestion: {user_input}\\nAnswer:\"\"\"\n",
    "\n",
    "    def inference(self):\n",
    "        if self.context_verbosity:\n",
    "            print(f\"Your Query: {self.full_prompt}\")\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the provided context to answer questions accurately.\"},\n",
    "            {\"role\": \"user\", \"content\": self.full_prompt}\n",
    "        ]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.llm.device)\n",
    "        generated_ids = self.llm.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=self.max_tokens,\n",
    "            top_k=self.top_k\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Customizing Your RAG LLM Chatbot Experience\n",
    "\n",
    "Welcome to the RAG LLM Chatbot interface! This guide will help you understand how to use the various widgets to customize your chatbot experience.\n",
    "\n",
    "#### Interface Elements\n",
    "\n",
    "##### Model Selection\n",
    "- **Model Dropdown**: Choose the model for your chatbot.\n",
    "  - Currently using the Falcon3-1B-Instruct model from TII.\n",
    "  - More models will be added in future updates.\n",
    "\n",
    "##### Query Input\n",
    "- **Query Text Box**: Enter your query here.\n",
    "  - Type the question or statement you want the chatbot to respond to.\n",
    "  - The text area provides ample space for longer queries.\n",
    "\n",
    "##### Response Customization\n",
    "- **Top K Slider**: Adjust the number of top results to consider for generating responses.\n",
    "  - Slide to increase or decrease the value. The range is from 1 to 4.\n",
    "  - This adjusts how many similar context passages are retrieved for answering.\n",
    "\n",
    "- **RAG OFF Checkbox**: Toggle whether to use Retrieval-Augmented Generation (RAG) or not.\n",
    "  - Check this box if you want to turn off RAG and use only the base model for responses.\n",
    "  - Useful for comparing RAG-enhanced vs. basic model responses.\n",
    "\n",
    "##### Data Processing Settings\n",
    "- **Chunk Size Input**: Set the size of text chunks for processing.\n",
    "  - Enter a value to determine how large each text chunk should be (5-5000).\n",
    "  - This affects how the context documents are segmented for retrieval.\n",
    "\n",
    "- **Overlap Input**: Define the overlap size between chunks.\n",
    "  - Set a value for how much overlap there should be between text chunks (0-1000).\n",
    "  - Higher overlap can help maintain context continuity between chunks.\n",
    "\n",
    "##### Dataset and Performance\n",
    "- **Dataset Dropdown**: Choose the dataset for context retrieval.\n",
    "  - Options include: 'robot maintenance', 'basketball coach', 'physics professor', 'grocery cashier'.\n",
    "  - Each dataset provides different domain-specific knowledge.\n",
    "\n",
    "- **Threads Slider**: Select the number of threads for processing.\n",
    "  - Adjust the slider to set the number of threads (2-200).\n",
    "  - Higher values may improve performance on multi-core systems.\n",
    "\n",
    "- **Max Tokens Input**: Specify the maximum length of generated responses.\n",
    "  - Enter a value to set the token limit (5-500).\n",
    "  - Higher values allow for longer, more detailed responses.\n",
    "\n",
    "##### Submit Button\n",
    "- **Submit**: Click this button to process your query and generate a response.\n",
    "  - The response will appear in a styled blue box below the interface.\n",
    "\n",
    "### How to Use\n",
    "1. Select your desired model from the **Model Dropdown**.\n",
    "2. Type your query in the **Query Text Box**.\n",
    "3. Set the **Top K Slider** for context retrieval amount.\n",
    "4. Configure **Chunk Size** and **Overlap** for text processing.\n",
    "5. Choose a dataset from the **Dataset Dropdown**.\n",
    "6. Adjust performance parameters (**Threads**, **Max Tokens**).\n",
    "7. Toggle **RAG OFF** if you want to test the base model.\n",
    "8. Click **Submit** to generate the response.\n",
    "\n",
    "> Note: The first execution will load the model into memory and may take longer. Subsequent queries will be faster unless you change model-specific parameters like threads or max tokens.nse.\r\n",
    "\r\n",
    "Enjoy interacting with your custom RAG LLM Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inputs(b):\n",
    "    \"\"\"\n",
    "    Process inputs from the interactive chat interface.\n",
    "    \"\"\"\n",
    "    global previous_threads, previous_max_tokens, previous_top_k, previous_dataset\n",
    "    global previous_chunk_size, previous_overlap\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        f = io.StringIO()\n",
    "        with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n",
    "            model = model_dropdown.value\n",
    "            query = query_text.value\n",
    "            top_k = top_k_slider.value\n",
    "            chunk_size = chunk_size_input.value\n",
    "            overlap = overlap_input.value\n",
    "            dataset = dataset_dropdown.value\n",
    "            threads = threads_slider.value\n",
    "            max_tokens = max_token_input.value\n",
    "            rag_off = rag_off_checkbox.value\n",
    "            \n",
    "            bot.get_model(model=model)\n",
    "            bot.download_dataset(dataset=dataset)\n",
    "            if (threads != previous_threads or max_tokens != previous_max_tokens or \n",
    "                top_k != previous_top_k):\n",
    "                print(\"Loading model with new parameters\")\n",
    "                bot.load_model(\n",
    "                    n_threads=threads,\n",
    "                    max_tokens=max_tokens,\n",
    "                    repeat_penalty=1.50,\n",
    "                    n_batch=threads,\n",
    "                    top_k=top_k\n",
    "                )\n",
    "                previous_threads = threads\n",
    "                previous_max_tokens = max_tokens\n",
    "                previous_top_k = top_k\n",
    "            \n",
    "            # Rebuild vector DB if needed\n",
    "            if (dataset != previous_dataset or chunk_size != previous_chunk_size or \n",
    "                overlap != previous_overlap):\n",
    "                print(\"Rebuilding vector DB\")\n",
    "                bot.build_vectordb(chunk_size=chunk_size, overlap=overlap)\n",
    "                previous_dataset = dataset\n",
    "                previous_chunk_size = chunk_size\n",
    "                previous_overlap = overlap\n",
    "            bot.retrieval_mechanism(\n",
    "                user_input=query, \n",
    "                top_k=2,\n",
    "                context_verbosity=True,\n",
    "                rag_off=rag_off\n",
    "            )\n",
    "            response = bot.inference()            \n",
    "            styled_response = f\"\"\"\n",
    "            <div style=\"\n",
    "                background-color: lightblue;\n",
    "                border-radius: 15px;\n",
    "                padding: 10px;\n",
    "                font-family: Arial, sans-serif;\n",
    "                color: black;\n",
    "                max-width: 600px;\n",
    "                word-wrap: break-word;\n",
    "                margin: 10px;\n",
    "                font-size: 14px;\">\n",
    "                {response}\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            display(HTML(styled_response))\n",
    "\n",
    "def create_chat_interface():\n",
    "    \"\"\"\n",
    "    Create and display the interactive chat interface widgets.\n",
    "    \"\"\"\n",
    "    global model_dropdown, query_text, top_k_slider, rag_off_checkbox\n",
    "    global chunk_size_input, overlap_input, dataset_dropdown\n",
    "    global threads_slider, max_token_input, output\n",
    "    \n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=['Falcon', 'More Models Coming Soon!'],\n",
    "        description='Model:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    query_layout = widgets.Layout(width='400px', height='100px')\n",
    "    query_text = widgets.Textarea(\n",
    "        placeholder='Type your query here',\n",
    "        description='Query:',\n",
    "        disabled=False,\n",
    "        layout=query_layout\n",
    "    )\n",
    "    top_k_slider = widgets.IntSlider(\n",
    "        value=2,\n",
    "        min=1,\n",
    "        max=4,\n",
    "        step=1,\n",
    "        description='Top K:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d'\n",
    "    )\n",
    "    rag_off_checkbox = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='RAG OFF?',\n",
    "        disabled=False,\n",
    "        indent=False,\n",
    "        tooltip='Turn off RAG and use only the base model'\n",
    "    )\n",
    "    chunk_size_input = widgets.BoundedIntText(\n",
    "        value=500,\n",
    "        min=5,\n",
    "        max=5000,\n",
    "        step=1,\n",
    "        description='Chunk Size:',\n",
    "        disabled=False\n",
    "    )\n",
    "    overlap_input = widgets.BoundedIntText(\n",
    "        value=50,\n",
    "        min=0,\n",
    "        max=1000,\n",
    "        step=1,\n",
    "        description='Overlap:',\n",
    "        disabled=False\n",
    "    )\n",
    "    dataset_dropdown = widgets.Dropdown(\n",
    "        options=['robot maintenance',\n",
    "                 'basketball coach',\n",
    "                 'physics professor',\n",
    "                 'grocery cashier'],\n",
    "        description='Dataset:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    threads_slider = widgets.IntSlider(\n",
    "        value=8,\n",
    "        min=2,\n",
    "        max=200,\n",
    "        step=1,\n",
    "        description='Threads:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d'\n",
    "    )\n",
    "    max_token_input = widgets.BoundedIntText(\n",
    "        value=50,\n",
    "        min=5,\n",
    "        max=500,\n",
    "        step=5,\n",
    "        description='Max Tokens:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    # Layout\n",
    "    left_column = widgets.VBox([\n",
    "        model_dropdown, \n",
    "        top_k_slider, \n",
    "        rag_off_checkbox,\n",
    "        chunk_size_input, \n",
    "        overlap_input, \n",
    "        dataset_dropdown, \n",
    "        threads_slider,\n",
    "        max_token_input\n",
    "    ])\n",
    "\n",
    "    submit_button = widgets.Button(description=\"Submit\")\n",
    "    submit_button.on_click(process_inputs)\n",
    "    right_column = widgets.VBox([query_text, submit_button])\n",
    "    interface_layout = widgets.HBox([left_column, right_column])\n",
    "    output = widgets.Output()\n",
    "    display(interface_layout, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = RAGBot()\n",
    "previous_threads = None\n",
    "previous_max_tokens = None\n",
    "previous_top_k = None\n",
    "previous_dataset = None\n",
    "previous_chunk_size = None\n",
    "previous_overlap = None\n",
    "previous_temp = None\n",
    "\n",
    "# Create and display the interface\n",
    "create_chat_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Disclaimer for Using Large Language Models\n",
    "\n",
    "Please be aware that while Large Language Models like Falcon are powerful tools for text generation, they may sometimes produce results that are unexpected, biased, or inconsistent with the given prompt. It's advisable to carefully review the generated text and consider the context and application in which you are using these models.\n",
    "\n",
    "Usage of these models must also adhere to the licensing agreements and be in accordance with ethical guidelines and best practices for AI. If you have any concerns or encounter issues with the models, please refer to the respective model cards and documentation provided in the links above.\n",
    "\n",
    "To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\n",
    "\n",
    " \n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\n",
    "\n",
    "Intel’s provision of these resources does not expand or otherwise alter Intel’s applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.7",
   "language": "python",
   "name": "pytorch27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
